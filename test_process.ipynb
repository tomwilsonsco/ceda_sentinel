{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Input a date range and a polygon for area(s) of interest\n",
    "- Find folders that are within that date range\n",
    "- Find images that intersect the polygons\n",
    "- Clip the images that intersect by the polygons and save geotiff of the interecting area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://data.ceda.ac.uk/neodc/sentinel_ard/data/sentinel_2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = \"2023-05-01\"\n",
    "end_date = \"2023-05-10\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_date_url(base_url, input_date):\n",
    "    year = input_date.strftime(\"%Y\")\n",
    "    month = input_date.strftime(\"%m\")\n",
    "    day = input_date.strftime(\"%d\")\n",
    "    return f\"{base_url}/{year}/{month}/{day}\"\n",
    "\n",
    "\n",
    "def get_existing_folders(base_url, start_date, end_date):\n",
    "    start_date = datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "    end_date = datetime.strptime(end_date, \"%Y-%m-%d\")\n",
    "    current_date = start_date\n",
    "    urls = []\n",
    "    while current_date <= end_date:\n",
    "        check_url = create_date_url(base_url, current_date)\n",
    "        response = requests.get(check_url, timeout=10)\n",
    "        if response.status_code == 200:\n",
    "            urls.append(check_url)\n",
    "        current_date += timedelta(days=1)\n",
    "    return urls\n",
    "\n",
    "\n",
    "def extract_xml_links(url):\n",
    "    \"\"\"Extracts XML links from the given HTML webpage URL.\"\"\"\n",
    "    xml_links = []\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "        for link in soup.find_all(\"a\", href=True):\n",
    "            href = link[\"href\"]\n",
    "            if href.endswith(\".xml?download=1\"):\n",
    "                xml_links.append(href)\n",
    "\n",
    "    return xml_links\n",
    "\n",
    "\n",
    "def all_xml_list(base_url, start_date, end_date):\n",
    "    date_urls = get_existing_folders(base_url, start_date, end_date)\n",
    "    xml_links = []\n",
    "    for url in date_urls:\n",
    "        xml_links.extend(extract_xml_links(url))\n",
    "    return xml_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "370\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['https://dap.ceda.ac.uk/neodc/sentinel_ard/data/sentinel_2/2023/05/01/S2A_20230501_latn500lonw0008_T30UXA_ORB137_20230501131147_utm30n_osgb_vmsk_sharp_rad_srefdem_stdsref_meta.xml?download=1',\n",
       " 'https://dap.ceda.ac.uk/neodc/sentinel_ard/data/sentinel_2/2023/05/01/S2A_20230501_latn500lonw0008_T30UXA_ORB137_20230501131147_utm30n_osgb_vmsk_sharp_rad_srefdem_stdsref_meta.xml?download=1']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xml_links = all_xml_list(base_url, start_date, end_date)\n",
    "print(len(xml_links))\n",
    "xml_links[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_xml(url):\n",
    "    # Input URL of the XML file\n",
    "    try:\n",
    "        # Send an HTTP GET request to the URL\n",
    "        response = requests.get(url)\n",
    "        # Check if the request was successful\n",
    "        if response.status_code == 200:\n",
    "            # Parse the XML content using BeautifulSoup\n",
    "            soup = BeautifulSoup(response.content, \"xml\")\n",
    "\n",
    "            # Find the ARCSI_CLOUD_COVER value using the tag name\n",
    "            arcsi_cloud_cover_element = soup.find(\"gco:CharacterString\")\n",
    "\n",
    "            # Check if the element was found and extract the value\n",
    "            if arcsi_cloud_cover_element is not None:\n",
    "                arcsi_cloud_cover = arcsi_cloud_cover_element.text\n",
    "                print(\"ARCSI_CLOUD_COVER:\", arcsi_cloud_cover)\n",
    "            else:\n",
    "                print(\"ARCSI_CLOUD_COVER not found in the XML.\")\n",
    "        else:\n",
    "            print(\"Failed to retrieve the XML. Status code:\", response.status_code)\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(\"Error:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in xml_links:\n",
    "    process_xml(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [502]>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geospatial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
