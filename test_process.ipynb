{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import lxml\n",
    "from datetime import datetime, timedelta\n",
    "from shapely.geometry import box"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Input a date range and a polygon for area(s) of interest\n",
    "- Find folders that are within that date range\n",
    "- Find images that intersect the polygons\n",
    "- Clip the images that intersect by the polygons and save geotiff of the interecting area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://data.ceda.ac.uk/neodc/sentinel_ard/data/sentinel_2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = \"2023-05-01\"\n",
    "end_date = \"2023-05-10\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_date_url(base_url, input_date):\n",
    "    year = input_date.strftime(\"%Y\")\n",
    "    month = input_date.strftime(\"%m\")\n",
    "    day = input_date.strftime(\"%d\")\n",
    "    return f\"{base_url}/{year}/{month}/{day}\"\n",
    "\n",
    "\n",
    "def get_existing_folders(base_url, start_date, end_date):\n",
    "    start_date = datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "    end_date = datetime.strptime(end_date, \"%Y-%m-%d\")\n",
    "    current_date = start_date\n",
    "    urls = []\n",
    "    while current_date <= end_date:\n",
    "        check_url = create_date_url(base_url, current_date)\n",
    "        response = requests.get(check_url, timeout=10)\n",
    "        if response.status_code == 200:\n",
    "            urls.append(check_url)\n",
    "        current_date += timedelta(days=1)\n",
    "    return urls\n",
    "\n",
    "\n",
    "def extract_xml_links(url):\n",
    "    \"\"\"Extracts XML links from the given HTML webpage URL.\"\"\"\n",
    "    xml_links = []\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "        for link in soup.find_all(\"a\", href=True):\n",
    "            href = link[\"href\"]\n",
    "            if href.endswith(\".xml?download=1\"):\n",
    "                xml_links.append(href)\n",
    "\n",
    "    return xml_links\n",
    "\n",
    "\n",
    "def all_xml_list(base_url, start_date, end_date):\n",
    "    date_urls = get_existing_folders(base_url, start_date, end_date)\n",
    "    xml_links = []\n",
    "    for url in date_urls:\n",
    "        xml_links.extend(extract_xml_links(url))\n",
    "    return xml_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "370\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['https://dap.ceda.ac.uk/neodc/sentinel_ard/data/sentinel_2/2023/05/01/S2A_20230501_latn500lonw0008_T30UXA_ORB137_20230501131147_utm30n_osgb_vmsk_sharp_rad_srefdem_stdsref_meta.xml?download=1',\n",
       " 'https://dap.ceda.ac.uk/neodc/sentinel_ard/data/sentinel_2/2023/05/01/S2A_20230501_latn500lonw0008_T30UXA_ORB137_20230501131147_utm30n_osgb_vmsk_sharp_rad_srefdem_stdsref_meta.xml?download=1']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xml_links = all_xml_list(base_url, start_date, end_date)\n",
    "print(len(xml_links))\n",
    "xml_links[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _extract_xml_cloud(xml_extract):\n",
    "    supp = xml_extract.find(\"gmd:supplementalinformation\")\n",
    "    character_string = supp.find(\"gco:characterstring\").text\n",
    "    lines = character_string.split(\"\\n\")\n",
    "    lines = [\"\".join(l.split()) for l in lines]\n",
    "    for line in lines:\n",
    "        if line.startswith(\"ARCSI_CLOUD_COVER\"):\n",
    "            arcsi_cloud_cover = line.split(\":\")[1].strip()\n",
    "            val = arcsi_cloud_cover\n",
    "            break\n",
    "    return float(val)\n",
    "\n",
    "\n",
    "def _clean_coord(coord):\n",
    "    coord = coord.replace(\"\\n\", \"\")\n",
    "    return float(coord)\n",
    "\n",
    "\n",
    "def _extract_extent(xml_extract):\n",
    "    minx = _clean_coord(xml_extract.find(\"gmd:westboundlongitude\").text)\n",
    "    miny = _clean_coord(xml_extract.find(\"gmd:southboundlatitude\").text)\n",
    "    maxx = _clean_coord(xml_extract.find(\"gmd:eastboundlongitude\").text)\n",
    "    maxy = _clean_coord(xml_extract.find(\"gmd:northboundlatitude\").text)\n",
    "    return minx, miny, maxy, maxy\n",
    "\n",
    "\n",
    "def _read_xml(url):\n",
    "    # Send an HTTP GET request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the XML content using BeautifulSoup with lxml parser\n",
    "        soup = BeautifulSoup(response.text, \"lxml\")\n",
    "    return soup\n",
    "\n",
    "\n",
    "def filter_xmls(xml_links, southern_most_lat=54, geometry=None, cloud_cover_max=0.4):\n",
    "    retained_links = []\n",
    "    for url in xml_links:\n",
    "        # read the xml\n",
    "        try:\n",
    "            xml_extract = _read_xml(url)\n",
    "        except:\n",
    "            continue\n",
    "        # first get the coords and see if image is too far south\n",
    "        coords = _extract_extent(xml_extract)\n",
    "        if coords[3] < southern_most_lat:\n",
    "            continue\n",
    "        # check if too cloudy overall\n",
    "        if _extract_xml_cloud(xml_extract) > cloud_cover_max:\n",
    "            continue\n",
    "        # finally if specified check extent intersects input geometry\n",
    "        if geometry is not None:\n",
    "            image_geom = box(coords)\n",
    "            if image_geom.intersects(geometry):\n",
    "                retained_links.append(url)\n",
    "        else:\n",
    "            retained_links.append(url)\n",
    "    return retained_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_xml_links = filter_xmls(xml_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filtered_xml_links)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geospatial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
